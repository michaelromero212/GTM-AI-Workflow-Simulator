# GTM AI Workflow Simulator

*Modeling how AI agents reduce friction across Sales, Customer Success, and Revenue Operations.*

GTM teams lose pipeline velocity to manual research, inconsistent deal inspection, and reactive post-sales workflows. This simulator demonstrates how AI agents can be deployed with governance, QA'd before rollout, measured against revenue KPIs, and iterated on using real field feedback â€” end-to-end, from intake to production.


## ðŸ“ˆ Revenue Impact (Simulated Environment)

| Metric | Result | How |
|--------|--------|-----|
| SDR Research Time | **~40% reduction** | Automated lead summarization replaces manual CRM + web research |
| AI Suggestion Acceptance | **68% â†’ 82%** | Structured prompt iteration with QA gates and feedback logging |
| Hallucination / Rule Violations | **11% â†’ 3%** | Governance constraints, escalation logic, and pre-deployment QA |
| CRM Data Completeness | **+15% improvement** | AI-generated structured call summaries with required field validation |

---

## ðŸ”„ Where AI Intervenes in the Revenue Lifecycle

| Stage | AI Workflow | KPI |
|-------|------------|-----|
| **Pipeline Generation** | Lead Brief Generator â€” automated research, qualification signals, next-best-action | Research Time, Conversion Rate |
| **Deal Execution** | Risk Signal Classifier â€” stalled deal detection, missing stakeholder alerts, competitive flags | Slipped Deals %, Forecast Accuracy |
| **Onboarding & Adoption** | Usage Insight Summary â€” customer health scoring, adoption milestone tracking | Time-to-Value, Expansion Signals |
| **Renewal & Expansion** | Health Risk Flags + Next Best Action â€” churn prediction, upsell/cross-sell surfacing | Renewal Rate, NRR Impact |

---

## ðŸŽ¯ What This Simulator Covers

- **Business Intent & Governance** â€” How AI agent behavior is defined, constrained, and audited
- **AI Agent Operations** â€” End-to-end agent implementation using LLMs (Hugging Face)
- **QA & Validation** â€” Pre-deployment testing, hallucination tracking, rule violation detection
- **KPIs & Analytics** â€” SQL-based metrics tied to revenue outcomes
- **Dashboards** â€” Operational health monitoring with interactive visualizations
- **Operational Diagnostics** â€” In-depth auditing and bottleneck analysis
- **A/B Testing** â€” Data-driven rollout decisions with statistical analysis
- **Enablement** â€” Rollout playbooks, training assets, and business reviews

## ðŸ—ï¸ Project Structure

```
Databricks-AI-Agent-Operations-&-GTM-Ops-Simulator/
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ .gitignore                         # Git exclusions
â”‚
â”œâ”€â”€ docs/                              # Business documentation
â”‚   â”œâ”€â”€ business_intent.md            # Agent definition & governance
â”‚   â”œâ”€â”€ kpis.md                       # KPI definitions
â”‚   â”œâ”€â”€ rollout_plan.md               # Deployment strategy
â”‚   â””â”€â”€ enablement.md                 # User guidance
â”‚
â”œâ”€â”€ agent/                             # AI Agent implementation
â”‚   â”œâ”€â”€ agent.py                      # Core agent logic
â”‚   â””â”€â”€ prompts.py                    # Prompt templates
â”‚
â”œâ”€â”€ qa/                                # Quality assurance
â”‚   â”œâ”€â”€ test_cases.json               # Structured test cases
â”‚   â””â”€â”€ run_qa.py                     # QA test runner
â”‚
â”œâ”€â”€ data/                              # Data storage
â”‚   â”œâ”€â”€ sample_agent_runs.csv         # Sample agent interaction data
â”‚   â””â”€â”€ uploaded_inputs/              # User-uploaded files
â”‚
â”œâ”€â”€ analytics/                         # Data analytics
â”‚   â”œâ”€â”€ load_data.py                  # Data ingestion (DuckDB)
â”‚   â””â”€â”€ queries.sql                   # SQL queries for KPIs
â”‚
â”œâ”€â”€ dashboards/                        # Visualization
â”‚   â””â”€â”€ kpi_dashboard.ipynb           # Jupyter dashboard
â”‚
â”œâ”€â”€ experiments/                       # A/B testing
â”‚   â””â”€â”€ ab_test.md                    # Experiment documentation
â”‚
â”œâ”€â”€ reviews/                           # Business reviews
â”‚   â””â”€â”€ weekly_business_review.md     # Review template
â”‚
â””â”€â”€ webapp/                            # Web application
    â”œâ”€â”€ app.py                        # FastAPI application
    â”œâ”€â”€ templates/                    # Jinja2 templates
    â”‚   â”œâ”€â”€ layout.html              # Shared layout with sidebar
    â”‚   â”œâ”€â”€ index.html               # Overview page
    â”‚   â”œâ”€â”€ dashboard.html           # KPI dashboard
    â”‚   â”œâ”€â”€ upload.html              # Test agent page
    â”‚   â””â”€â”€ explorer.html            # Operational Diagnostics
    â””â”€â”€ static/
        â””â”€â”€ styles.css               # Premium CSS design system
```

## ðŸš€ Quick Start

### 1. Clone & Setup

```bash
# Clone the repository
git clone https://github.com/michaelromero212/GTM-AI-Workflow-Simulator.git
cd GTM-AI-Workflow-Simulator

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # macOS/Linux
# venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt
```

### 2. Configure Hugging Face Token (Optional)

```bash
export HF_TOKEN="your_huggingface_token_here"
```

Get a free token from [Hugging Face](https://huggingface.co/settings/tokens). The agent works without a token using mock responses.

### 3. Run the Web Application

```bash
cd webapp
HF_TOKEN=your_token uvicorn app:app --host 0.0.0.0 --port 8000
```

Open your browser to: **http://localhost:8000**

## ðŸ“¸ Screenshots

### Performance Overview (Home Page)
The landing page shows real-time KPIs, A/B test comparisons, lead source performance, and recent activityâ€”all with interactive Chart.js visualizations.

![Home Page](docs/images/home_page.jpg)

### KPI Dashboard
Comprehensive metrics including Task Accuracy (92%), User Satisfaction (4.18/5), Resolution Time, and Error Rate. Interactive bar charts compare Agent A vs Agent B performance.

![KPI Dashboard](docs/images/dashboard_kpis.jpg)

### Operational Diagnostics
Audit the `agent_runs` data directly. Includes preset buttons for common analyses (Executive Summary, Failure Audit, etc.) and a full data catalog reference.

![Diagnostics Page](docs/images/sql_explorer.jpg)

### Test Agent
Upload lead data, deal notes, or sales inputs to test the AI agent. Select a task type and receive real LLM-powered analysis with feedback buttons.

![Upload Page](docs/images/upload_page.jpg)

### AI Agent Response
Real AI-generated analysis from the Llama-3.2 model. Includes Company Overview, Industry & Use Case Fit, Suggested Questions, and Recommended Next Steps.

![Agent Response](docs/images/agent_response.jpg)

## ðŸŽ¯ Role Alignment: GTM Systems & AI Engineer

This project demonstrates core competencies for **GTM AI** roles:

| Competency | Demonstration |
|------------|---------------|
| **0â†’1 Delivery** | Took ambiguous GTM pain points and shipped working AI workflows end-to-end |
| **Intake & Prioritization** | Defined requirements, success metrics, and backlog prioritization in `docs/` |
| **Cross-Functional Partnership** | Designed for Sales, CS, Ops, and Analytics stakeholders |
| **Governance & Guardrails** | QA gates, escalation rules, access controls, auditability from day one |
| **A/B Testing & Iteration** | Data-driven rollout decisions with Agent A vs B comparison |
| **Enablement & Rollout** | Training assets, office hours templates, comms plans |
| **Measurable Outcomes** | Every workflow tied to a revenue KPI with tracking infrastructure |

## ðŸš€ Key Features

### ðŸ¤– AI Agent
- Accepts GTM tasks: lead summaries, follow-up suggestions, deal risk signals
- Uses Hugging Face LLMs (Llama-3.2-3B-Instruct)
- Implements governance rules and escalation logic
- Falls back to mock responses without token

### ðŸ“‰ Operational Diagnostics
- **Overview**: Quick KPI snapshot, A/B comparison charts, recent activity
- **Dashboard**: Detailed metrics, trends, task type breakdown
- **Diagnostics**: Auditing console with preset diagnostic templates
- **Live Updates**: Every test agent run updates the dashboards

### ðŸ”¬ Operational Auditing
Run diagnostic reports directly against the agent data:
```sql
SELECT lead_source, COUNT(*) as volume, 
       ROUND(AVG(CASE WHEN user_accepted THEN 1.0 ELSE 0.0 END) * 100, 1) as accuracy
FROM agent_runs 
GROUP BY lead_source
ORDER BY accuracy DESC
```

### ðŸ§ª A/B Testing
- Real-time comparison of Agent A (control) vs Agent B (experimental)
- Visual charts showing accuracy, satisfaction, and error rates
- Decision recommendations based on success criteria
- Detailed analysis in `experiments/ab_test.md`

### âœ… QA & Validation
- Structured test cases in JSON
- Automated test runner
- Accuracy, hallucination, and rule violation tracking
- Results feed into analytics pipeline

## ðŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|------------|
| Backend | FastAPI + Python |
| Frontend | Jinja2 + Chart.js |
| Database | DuckDB (in-memory SQL) |
| LLM | Hugging Face Inference API |
| Styling | Custom CSS (Databricks-inspired) |
| Data Format | CSV with live appends |

## ðŸ›¡ï¸ Governance & Compliance by Design

Governance is built into every workflow, not bolted on after deployment:

- **Agent Constraints & Guardrails** â€” No autonomous external communication, no unauthorized data access, no strategic decisions without human review ([business_intent.md](docs/business_intent.md))
- **Escalation & Abstention Rules** â€” Agent automatically defers to humans for high-risk scenarios, ambiguous situations, and sensitive topics
- **QA Gates** â€” Structured test cases with hallucination detection, rule violation tracking, and accuracy thresholds before any rollout
- **Auditability** â€” Every agent run logged with user acceptance, confidence score, error flags, and version tracking
- **Privacy & Data Controls** â€” No PII in analytics, anonymized user IDs, configurable data retention policies
- **A/B Test Guardrails** â€” Auto-disable triggers if error rate > 10% or satisfaction < 2.5

## ðŸ”’ Security & Best Practices

- âœ… Secrets via environment variables (no credentials in code)
- âœ… Input validation and SQL injection prevention
- âœ… Virtual environment isolation
- âœ… Rate limiting and security headers
- âœ… Professional error handling

## ðŸ§ª Running QA Tests

```bash
python qa/run_qa.py
```

## ðŸ“ˆ Viewing Analytics (CLI)

```bash
python analytics/load_data.py
```

This loads data into DuckDB and executes queries from `analytics/queries.sql`.

## ðŸ“ Documentation

See the `docs/` directory for detailed documentation:

- **business_intent.md** - Agent definition, rules, and success criteria
- **kpis.md** - Metric definitions and tracking methodology
- **rollout_plan.md** - Phased deployment strategy
- **enablement.md** - User training and guidance
- **experiments/ab_test.md** - A/B test methodology and results

## ðŸ¤ Contributing

This is a demonstration project. Feel free to fork and adapt for your own use cases.

## ðŸ“„ License

MIT License - See LICENSE file for details

---

**Built to demonstrate how AI workflows are shipped, governed, and measured inside revenue organizations.**

*Last Updated: February 2026*
